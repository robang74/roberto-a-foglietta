<!DOCTYPE html>
<html>
    <head>
        <title>333-the-dilemma-ai-vs-human-decision-making</title>
        <meta charset='UTF-8'>
        <meta name='viewport' content='width=device-width, initial-scale=1.0'>
        <link rel='shortcut icon' type='image/x-icon' href='favicon.ico?'>
        <link rel='stylesheet' href='default.css'>
        <link rel='stylesheet' href='../intl/intlflg.css'>
        <!-- here begins the Javascript... why for the hell I got here? //-->
        <meta http-equiv='Content-Script-Type' content='text/javascript'>
        <link rel='stylesheet' href='ucustom.css' id='customStylesheet' media='screen'>
        <script>const cssdir='';</script note='global variable'>
        <script src='css-style-changer.js' defer></script>
        <link rel='stylesheet' href='printer.css' media='print'>
    </head>
    <body class=body>
<style>#printlink { display: inline; } @page { size: legal; margin: 0.50in 13.88mm 0.50in 13.88mm; zoom: 100%; } @media print { html { zoom: 100%; } }</style>
<p class='topbar'></p>
<div class='topbar' width='800px' translate='no'><b id='menu' onClick='nextStylesheet()'>&thinsp;&#9783;&thinsp;&Ropf;</b> &thinsp;&mdash;&thinsp; &#8543;&#8239;release: <b class='topbar'>2025-08-09&nbsp;<sup class='date-type topbar' id='datenote'>(&hairsp;<a href='#date-legenda' class='date-type topbar'>1</a>&hairsp;)</sup></b>  &thinsp;&mdash;&thinsp; rev.: <b class='topbar
'>2</b rev_num='
'> &thinsp;&mdash;&thinsp; transl.:&nbsp; <a class='topbar' href='https://robang74-github-io.translate.goog/roberto-a-foglietta/html/333-the-dilemma-ai-vs-human-decision-making?_x_tr_sl=en&_x_tr_tl=it&_x_tr_hl=it-IT&_x_tr_pto=wapp' target='_blank' rel='noopener noreferrer'>IT</a> &nbsp;<b>&middot;</b>&nbsp; <a class='topbar' href='https://robang74-github-io.translate.goog/roberto-a-foglietta/html/333-the-dilemma-ai-vs-human-decision-making?_x_tr_sl=en&_x_tr_tl=de&_x_tr_hl=de-DE&_x_tr_pto=wapp' target='_blank' rel='noopener noreferrer'>DE</a> &nbsp;<b>&middot;</b>&nbsp; <a class='topbar' href='https://robang74-github-io.translate.goog/roberto-a-foglietta/html/333-the-dilemma-ai-vs-human-decision-making?_x_tr_sl=en&_x_tr_tl=fr&_x_tr_hl=fr-FR&_x_tr_pto=wapp' target='_blank' rel='noopener noreferrer'>FR</a> &nbsp;<b>&middot;</b>&nbsp; <a class='topbar' href='https://robang74-github-io.translate.goog/roberto-a-foglietta/html/333-the-dilemma-ai-vs-human-decision-making?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es-ES&_x_tr_pto=wapp' target='_blank' rel='noopener noreferrer'>ES</a> &thinsp;&mdash;&thinsp; goto:&nbsp; <a class='topbar' href='../index.html#index'>.&#x27F0;.</a> &nbsp;<b>&middot;</b>&nbsp; <a class='topbar' href='../../chatbots-for-fun/index.html'target=_blank>C4F</a> &nbsp;<b>&middot;</b>&nbsp; <a class='topbar' href='../../chatgpt-answered-prompts/index.html'target=_blank>Q&A</a> <span id='printlink'>&thinsp;&mdash;&thinsp; <b>⎙</b>&hairsp;: <a aria-label='print this page' class='topbar' href='javascript:window.print()'>PDF</a></span>&nbsp;</div>
<div id="firstdiv">
<p class='topbar'></p>
<div align="center"><img class="wbsketch darkinv" src="../img/333-the-dilemma-ai-vs-human-decision-making-img-001.jpg" width="800"><br></div>
<p></p>
<H2 id="the-dilemma-ai-vs-human-decision-making">The dilemma AI vs Human decision making</H2>
<p></p>
<li><b>1st draft</b> &mdash; WORKING IN PROGRESS &mdash; some part of this article has been written as comments 2 days before.</li>
<p></p>
<hr>
<p></p>
<H3 id="humans-are-not-accountable-anyore-as-well">Humans are not accountable anyore, as well</H3>
<p></p>
In a world in which humans are not anymore accountable, nor trustworthy, machines are earning a place as decision makers. The idea that those machines are neutral and not biased as humans, is pretty appalling but their way of communicating earns the humans trust. 
<p></p>
And such a way, did not happen for a chance but for marketing thus it is also part of the plot. Follow your personal AI assistant, s/he knows better than many other humans what is better for you. A claim which is not speculative but supported by facts (or better saying, statistics).
<p></p>
<li>https://www.linkedin.com/posts/robertofoglietta_il-consulente-finanziario-un-altro-lavoro-activity-7359336445733494785-wWw2</li>
<p></p>
The agentic AI is going to commoditize a lot of process that even when they were not routines, they can be packed inside the "bureaucracy" category. The best is that all our process would be bureaucratic-free. Unfortunately, many forms of bureaucracy are still a business for many people. Hence, before strip bureaucracy, we need to commoditised it.
<p></p>
<blockquote>human judgment and ethics.</blockquote>
<p></p>
Human judgment is a nightmare and humans have no ethics, but pretend to have a moral superiority. More we talk about this topic and more I justify those wish to put the AI in concurrence with humans in taking decisions. Worse than humans, is difficult to do. In fact, stats say so.
<p></p>
<blockquote>Give the computer consciousness so that it can be held accountable. Problem solved.</blockquote>
<p></p>
Problem solved, until it claims his own rights!
<p></p>
<hr class="post-it">
<p></p>
<blockquote>only humans can be held to account. </blockquote>
<p></p>
Utopic, and unrealistic.
<p></p>
<li><a href="https://g.co/gemini/share/e7076c8fcccb" target='_blank' rel='noopener noreferrer'>g.co/gemini/share/e7076c8fcccb</a></li>
<p></p>
The paper’s core ideas, while spanning 47 pages, boil down to two principles: human-in-the-loop decision-making and human-AI co-evolution. This paper uses of academic jargon like "hermeneutic" could alienate average users, exposing a gap between its theoretical idealism and real-world practicality.
<p></p>
The most critical conclusion was that the paper fails to solve the "responsibility gap". Because accountability cannot be enforced on humans, as well.
<p></p>
For example, humans could use an external AI to evade accountability, making the oversight process an endless loop rather than a true co-evolution. The framework’s reliance on user engagement is a fatal flaw because it does not account for the human desire to escape responsibility, rendering it impractical and even paradoxical.
<p></p>
<!--//
<p></p>
Da una prima lettura veloce direi che ha scritto 47 pagine di opinioni basate su pochi casi reali portati ad esempio per affermare due concetti: 1. human in the loop; 2. l'ipotesi che umani e AI si evolvano insieme. 
<p></p>
Ma perché dovrebbero farlo? una domanda che un'AGI si porrebbe sicuramente. La risposta a questa domanda sta in questo link.
<p></p>
https://robang74.github.io/chatgpt-answered-prompts/html/a-journey-from-humans-ethics-to-ai-faith.html
<p></p>
Qualora MAI, un'AGI autentica emergesse (cosa che dubito assai) allora dalla bontà della risposta a quella domanda (link) dipende il futuro della nostra specie. Comunque pochissimi di noi umani sono attualmente in grado di evolvere mentalmente alla velocità delle AI, anche adesso che non sono AGI. La ragione sta scritta qui:
<p></p>
https://robang74.github.io/chatgpt-answered-prompts/html/propaganda-e-sistema-educativo.html
<p></p>
o meglio quella è la causa primaria, poi la radice del problema invece sta scritta qui
<p></p>
https://robang74.github.io/roberto-a-foglietta/html/324-il-modello-otto-novecentesco-ha-fallito.html
<p></p>
Comunque gli italiani sono "fottuti alla grande" visto che prediligono la furbizia all'intelligenza e quando si tratta di AGI, il trucco della furbizia dura molto poco.
<p></p>
//-->
<p></p>
<hr class="post-it">
<p></p>
<blockquote>That is my big question, what are you going to do, fire the AI Agent(s) who made a poor decision?</blockquote>
<p></p>
Let me put this in perspective: are we going to fire (roast would be better, anyway) politicians because they made poor decisions?
<p></p>
Sometimes, it happens. Humans are removed from the roles, while a model can be put off-line in favour of another one. I do not see the difference here, because it is hard to do in both cases. How long do you think that OpenAI would take to restore GPT-4?
<p></p>
However, as per rule of thumb. Education or re-education is the main point. In an ideal world, this would work. In the real world our education system is still tuned with the Industrial Revolution goals in mind. People who can read/write and handle a mechanical tool.
<p></p>
So the BIG question here (&ast;) is HOW we can manage to train (re-educate) millions, hundred million of people, when we ARE not even being able to get out that system from '800?
<p></p>
Under this perspective: 1. human always in the loop; 2. machine never can be considered accountable (aka take decisions); are theoretically granted. 
<p></p>
Reality tells us another story: people let machine decides for them, more and more.
<p></p>
AI companies should be considered accountable? Uhm...
<p></p>
<hr class="post-it">
<p></p>
<blockquote>Human in the loop has perfect sense. </blockquote>
<p></p>
Despite this many people inevitably will rely on AI outputs, as they are used to do with newspapers, radio, TV, social media, etc. How many of them? 
<p></p>
Something near 98% (Land, 1992) in the long run but 96% is the theoretical estimation without the extreme pressure on the far right tail of the gaussian. 
<p></p>
<hr class="post-it">
<p></p>
<blockquote>The person who signs the contract, agreement for management decision needs to be held accountable because the documents are signed by them.</blockquote>
<p></p>
Nice idea. Then, we might discover that in Italy who signs "certain" kind of contract is an idiot (whenever not even a "testa di legno" aka a puppet).
<p></p>
Demming were saying that 94% of the problem are "internal", many people who cite him did not understood that also 94% of the problems are "invisible" because they are fishes who swim in their own water (their own piss, it is a better formulation).
<p></p>
This is the MAIN reason because some other than fishes need an aquarium and why (of many others things). For example: who is in charge for QA (quality assurance) or best-practice? why nobody raises an exception? How a mistake made to be owned so long?
<p></p>
TCMO: total cost of ownership explain the concept, but cannot do anything against the people attitude to swim in their own piss!
<p></p>
<hr class="post-it">
<p></p>
<blockquote>We confuse pattern recognition with judgment, and those aren’t the same skill.</blockquote>
<p></p>
True! Until, reasoning gets in the scene and it makes no difference that it is a lexical or symbolic reasoning rather than a deep-thinking reasoning. Why, understanding does not matter?
<p></p>
<li>It is easier to judge than understand, thus people condemn!</li>
<p></p>
Never forget that "Barabba free" was one among those choice humans made, and not because someone (a single one) made a mistake but in group!
<p></p>
<blockquote>If we hand over decisions without retaining accountability, we’re not using AI as a partner; we’re abdicating our role in shaping the future</blockquote>
<p></p>
Totally agree, however 98% of the people will abdicating and the other 2% (by theory should have been 4%, and in the best scenario no more than 20%) will struggle with unilateral undebatable changes of the AI models (or their functioning) like in this example.
<p></p>
https://www.linkedin.com/posts/robertofoglietta_when-to-rag-by-guy-ernest-httpslnkdin-activity-7360535565705850881-9KE8
<p></p>
While we cannot enforce accountability and understanding over people, it would be better asking for a solid and stable NL API for chatbots (early entry point in AI) and transparence, at least.
<p></p>
Do what we can, the rest will come.
<p></p>
<hr class="post-it">
<p></p>
<blockquote>It is worse than that... we do not even know the identity of the AI systems used. Without identity there is no possibility of accountability.</blockquote>
<p></p>
In short: transparency.
<p></p>
It is not even a problem of "identification" of the model and last update time. Before knowing the weights (open models) which serves a little at this time because almost all the people capable of extracting some fact-driven conclusion from 200-300B parameters are for sure busy in something else more valuable (for them).
<p></p>
So, even before reach the "open models" paradigma, also the system prompt is not accessible to the users. Even before debate why a session-prompt like Katia has not a stable right to run, not even on a chosen specific configuration.
<p></p>
=-> lnkd.in/dWv5-sKm (persistent cache)
<p></p>
It is not even a mere question about what are the "forbidden topics" that the chatbot admitted to have but refused to list.
<p></p>
=-> lnkd.in/d_yjty2z (forbidden topics)
<p></p>
It is also about specific rules like:
<p></p>
1 - shit the output of the accounts listed in this database (A)
2 - open a supervising console for accounts in database (B)
<p></p>
Which are (1) a form of censorship + persecution and (2) know-how exfiltration, both political, and industrial.
<p></p>
<hr class="post-it">
<p></p>
<blockquote>Excellent point. The trolley problem.</blockquote>
<p></p>
The trolley dilemma, in short: 
<p></p>
<li>1. humans do not take "decision" under that conditions unless they are jet fighting pilots with cold-blood and trained to quick-thinking reactions (blinking). </li>
<p></p>
<li>2. while AI can have the time to retrieve and ponderate over HUGE amount of information. In both cases, no ethics is involved.</li>
<p></p>
<H3 id="1-how-humans-act">1. How Humans Act</H3>
<p></p>
=-> lnkd.in/dAgWzv-t
<p></p>
Regarding instinctive choices in dangerous situations, it's important to remember that adrenaline surges sideline the cortex in decision-making, giving almost exclusive priority to the hypothalamus. This process is almost instantaneous, on a human time scale, and excludes all higher and secondary cognitive functions.
<p></p>
<H3 id="2-what-ai-can-ponderate">2. What AI can ponderate</H3>
<p></p>
=-> lnkd.in/dWqjVZj6
<p></p>
There are several ways to take a decision in such a scenario, all of them are about minising the value of a multivariable function (aka local minimum in a field): lives lost, car occupants damage, insurance damage to pay (!!), minimise the action (physic), save the President (!!).
<p></p>
Does every life worth the same? Nope, it is harsh to admit and hard to negate. Moreover, it is highly hypocrite thinking that a company could sell cars keen to kill the occupants.
<p></p>
<br>
<p></p>
<H2 id="share-alike">Share alike</H2>
<p></p>
<p>&copy; 2025, <b>Roberto A. Foglietta</b> &lt;roberto.foglietta<span>&commat;</span>gmail.com&gt;, <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target='_blank' rel='noopener noreferrer'>CC BY-NC-ND 4.0</a></p>
<p></p>
</div>
<div id='date-legenda' align='center' translate='no'><sub><hr></sub><sub><b>date legenda</b>: &#x2776; first draft publishing date or &#x2777; creation date in git, otherwise &#x2778; html creation page date. <u>&mapstoup;<a href='#' class='toplink' translate='no'>top</a>&mapstoup;</u></sub></div>
<br class='pagebreak'>
    </body>
</html>
