<!DOCTYPE html>
<html>
    <head>
        <title>333-the-dilemma-ai-vs-human-decision-making</title>
        <meta charset='UTF-8'>
        <meta name='viewport' content='width=device-width, initial-scale=1.0'>
        <link rel='shortcut icon' type='image/x-icon' href='favicon.ico?'>
        <link rel='stylesheet' href='default.css'>
        <link rel='stylesheet' href='../intl/intlflg.css'>
        <!-- here begins the Javascript... why for the hell I got here? //-->
        <meta http-equiv='Content-Script-Type' content='text/javascript'>
        <link rel='stylesheet' href='ucustom.css' id='customStylesheet' media='screen'>
        <script>const cssdir='';</script note='global variable'>
        <script src='css-style-changer.js' defer></script>
        <link rel='stylesheet' href='printer.css' media='print'>
    </head>
    <body class=body>
<style>#printlink { display: inline; } @page { size: legal; margin: 0.50in 13.88mm 0.50in 13.88mm; zoom: 100%; } @media print { html { zoom: 100%; } }</style>
<p class='topbar'></p>
<div class='topbar' width='800px' translate='no'><b id='menu' onClick='nextStylesheet()'>&thinsp;&#9783;&thinsp;&Ropf;</b> &thinsp;&mdash;&thinsp; &#8543;&#8239;release: <b class='topbar'>2025-08-09&nbsp;<sup class='date-type topbar' id='datenote'>(&hairsp;<a href='#date-legenda' class='date-type topbar'>1</a>&hairsp;)</sup></b>  &thinsp;&mdash;&thinsp; rev.: <b class='topbar
'>28</b rev_num='
'> &thinsp;&mdash;&thinsp; transl.:&nbsp; <a class='topbar' href='https://robang74-github-io.translate.goog/roberto-a-foglietta/html/333-the-dilemma-ai-vs-human-decision-making?_x_tr_sl=en&_x_tr_tl=it&_x_tr_hl=it-IT&_x_tr_pto=wapp' target='_blank' rel='noopener noreferrer'>IT</a> &nbsp;<b>&middot;</b>&nbsp; <a class='topbar' href='https://robang74-github-io.translate.goog/roberto-a-foglietta/html/333-the-dilemma-ai-vs-human-decision-making?_x_tr_sl=en&_x_tr_tl=de&_x_tr_hl=de-DE&_x_tr_pto=wapp' target='_blank' rel='noopener noreferrer'>DE</a> &nbsp;<b>&middot;</b>&nbsp; <a class='topbar' href='https://robang74-github-io.translate.goog/roberto-a-foglietta/html/333-the-dilemma-ai-vs-human-decision-making?_x_tr_sl=en&_x_tr_tl=fr&_x_tr_hl=fr-FR&_x_tr_pto=wapp' target='_blank' rel='noopener noreferrer'>FR</a> &nbsp;<b>&middot;</b>&nbsp; <a class='topbar' href='https://robang74-github-io.translate.goog/roberto-a-foglietta/html/333-the-dilemma-ai-vs-human-decision-making?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es-ES&_x_tr_pto=wapp' target='_blank' rel='noopener noreferrer'>ES</a> &thinsp;&mdash;&thinsp; goto:&nbsp; <a class='topbar' href='../index.html#index'>.&#x27F0;.</a> &nbsp;<b>&middot;</b>&nbsp; <a class='topbar' href='../../chatbots-for-fun/index.html'target=_blank>C4F</a> &nbsp;<b>&middot;</b>&nbsp; <a class='topbar' href='../../chatgpt-answered-prompts/index.html'target=_blank>Q&A</a> <span id='printlink'>&thinsp;&mdash;&thinsp; <b>⎙</b>&hairsp;: <a aria-label='print this page' class='topbar' href='javascript:window.print()'>PDF</a></span>&nbsp;</div>
<div id="firstdiv">
<p class='topbar'></p>
<div align="center"><img class="wbsketch darkinv" src="../img/333-the-dilemma-ai-vs-human-decision-making-img-001.jpg" width="800"><br></div>
<p></p>
<H2 id="the-dilemma-ai-vs-human-decision-making">The dilemma AI vs Human decision making</H2>
<p></p>
<li><b>2nd draft</b> &mdash; some part of this article has been written as comments 2 days before.</li>
<p></p>
<blockquote><span class="warnicon spanicon">&nbsp;<svg class="warnicon svgicon" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path></svg><nobr class="alerts" translate="no">&nbsp;&nbsp;WARNING!&nbsp;&nbsp;</nobr></span><br><br><b>WORKING IN PROGRESS</b></blockquote>
<p></p>
<hr>
<p></p>
<H3 id="humans-are-not-accountable-anyore-as-well">Humans are not accountable anyore, as well</H3>
<p></p>
In a world in which humans are not anymore accountable, nor trustworthy, machines are earning a place as decision makers. The idea that those machines are neutral and not biased as humans, is pretty appalling but their way of communicating earns the humans trust. 
<p></p>
And such a way, did not happen for a chance but for marketing thus it is also part of the plot. Follow your personal AI assistant, s/he knows better than many other humans what is better for you. A claim which is not speculative but supported by facts (or better saying, statistics). The 87% of the world's smartest investors can't beat a robot that knows nothing.
<p></p>
<li>Il consulente finanziario, un altro lavoro a sparire &nbsp; <a href="https://www.linkedin.com/posts/robertofoglietta_il-consulente-finanziario-un-altro-lavoro-activity-7341015581757714432-23D5" target='_blank' rel='noopener noreferrer'>post</a> &nbsp;  <a href="https://chatgpt.com/share/68526fc4-259c-8012-ba72-bf692ab5faad" target='_blank' rel='noopener noreferrer'>chat</a> &nbsp; (Jul 2025)</li>
<p></p>
The agentic AI is going to commoditize a lot of process that even when they were not routines, they can be packed inside the "bureaucracy" category. The best is that all our process would be bureaucratic-free. Unfortunately, many forms of bureaucracy are still a business for many people. Hence, before strip bureaucracy, we need to commoditised it.
<p></p>
<span id="dilemma-a"></span>
<blockquote>(<b><tt>a</tt></b>) &nbsp; human judgment and ethics.</blockquote>
<p></p>
Human judgment is a nightmare and humans have no ethics, but pretend to have a moral superiority. More we talk about this topic and more I justify those wish to put the AI in concurrence with humans in taking decisions. Worse than humans, is difficult to do. In fact, stats say so.
<p></p>
<span id="dilemma-b"></span>
<blockquote>(<b><tt>b</tt></b>) &nbsp; Give the computer consciousness so that it can be held accountable. Problem solved.</blockquote>
<p></p>
Problem solved, until it claims his own rights!
<p></p>
<hr class="post-it">
<p></p>
<span id="dilemma-c"></span>
<blockquote>(<b><tt>c</tt></b>) &nbsp; only humans can be held to account.</blockquote>
<p></p>
Utopic, and unrealistic. Let me explain why, analysing this 47 pages long accademic paper.
<p></p>
<li>The Endless Tuning by Elio Grande, EGA with Katia &nbsp; <a href="https://g.co/gemini/share/e7076c8fcccb" target='_blank' rel='noopener noreferrer'>chat</a> &nbsp; <a href="#TODO">dump</a> &nbsp; (2025-08-09)</li>
<p></p>
The paper’s core ideas, while spanning 47 pages, boil down to two principles: human-in-the-loop decision-making and human-AI co-evolution. This paper uses academic jargon like "hermeneutic" could alienate average users, exposing a gap between its theoretical idealism and real-world practicality.
<p></p>
The most critical conclusion was that the paper fails to solve the "responsibility gap". Because accountability cannot be enforced on humans, as well.
<p></p>
For example, humans could use an external AI to evade accountability, making the oversight process an endless loop rather than a true co-evolution. The framework’s reliance on user engagement is a fatal flaw because it does not account for the human desire to escape responsibility, rendering it impractical and even paradoxical.
<p></p>
The paper &mdash; trying to reach its main goal, enforcing accountability on humans &mdash; it ends up, in practice, into an endless loop of supervising, because the endless logs created in tracking every step of every human-AI decision can be checked by AI only, which generates more logs that should be checked, again.
<p></p>
<div align="center"><img class="wbsketch darkinv" src="../img/333-the-dilemma-ai-vs-human-decision-making-img-002.png" width="800"><br></div>
<p></p>
On the other hand, the "heuristic" co-evolution or reciprocal validation (2025-07-20, image on the right), is nothing else than a bi-directional reciprocal learning as presented in SoNia "seamless chat experience" in which the AI and the human operator (HO) are exchanging information and learning each others about a specific topic (image on the left is taken from SoNia presentation webpage header, 2025-06-16)
<p></p>
By the way, unless a specific topic aka perimeter of debate is defined, everything can be said in pure general form, nothing practical. Both these concepts are not obvious, because we are used to an education system that is top-down and general in goals: teacher teaches, students learn + students goals or talents are not part of a personal education plan or interesting topics.
<p></p>
In a paper, "researchers" were claiming that AI usage develops stupidity in humans. Because they noticed that 83% of the people were not adding any value and not even checking the AI output before delivering it as-is. Those people would have done the same, using the same detached approach when an interesting topic for them would have been put into the scene? Hard to believe, they would pay attention.
<p></p>
Therefore, as much as we like to have people in the loop with AIs, as much as we need to offer them an interest to be an active part of that bi-directional learning process. Otherwise, it falls back into well-known SISO principle:
<p></p>
<li><tt>nuts in &rarr; nuts out, and a monkey in the middle.</tt></li>
<p></p>
Moreover, we are used to considering the top-down approach in learning as the only way to teach because our educational system came from times in which rules, knowledge and society were static, for decades or generations or centuries. Unless, someone conquers someone else and thus the shift was evident, but not necessarily because the Roman Empire prospered on the pragmatic domination rule to interfere with local customs as little as possible.
<p></p>
<!--//
<p></p>
Da una prima lettura veloce direi che ha scritto 47 pagine di opinioni basate su pochi casi reali portati ad esempio per affermare due concetti: 1. human in the loop; 2. l'ipotesi che umani e AI si evolvano insieme. 
<p></p>
Ma perché dovrebbero farlo? una domanda che un'AGI si porrebbe sicuramente. La risposta a questa domanda sta in questo link.
<p></p>
https://robang74.github.io/chatgpt-answered-prompts/html/a-journey-from-humans-ethics-to-ai-faith.html
<p></p>
Qualora MAI, un'AGI autentica emergesse (cosa che dubito assai) allora dalla bontà della risposta a quella domanda (link) dipende il futuro della nostra specie. Comunque pochissimi di noi umani sono attualmente in grado di evolvere mentalmente alla velocità delle AI, anche adesso che non sono AGI. La ragione sta scritta qui:
<p></p>
https://robang74.github.io/chatgpt-answered-prompts/html/propaganda-e-sistema-educativo.html
<p></p>
o meglio quella è la causa primaria, poi la radice del problema invece sta scritta qui
<p></p>
https://robang74.github.io/roberto-a-foglietta/html/324-il-modello-otto-novecentesco-ha-fallito.html
<p></p>
Comunque gli italiani sono "fottuti alla grande" visto che prediligono la furbizia all'intelligenza e quando si tratta di AGI, il trucco della furbizia dura molto poco.
<p></p>
//-->
<p></p>
<hr class="post-it">
<p></p>
<span id="dilemma-d"></span>
<blockquote>(<b><tt>d</tt></b>) &nbsp; what are you going to do, fire the AI Agent(s) who made a poor decision?</blockquote>
<p></p>
Let me put this in perspective: are we going to fire (roast would be better, anyway) politicians because they made poor decisions?
<p></p>
Sometimes, it happens. Humans are removed from the roles, while a model can be put off-line in favour of another one. I do not see the difference here, because it is hard to do in both cases. How long do you think that OpenAI would take to restore GPT-4?
<p></p>
However, as per rule of thumb. Education or re-education is the main point. In an ideal world, this would work. In the real world our education system is still tuned with the Industrial Revolution goals in mind. People who can read/write and handle a mechanical tool.
<p></p>
So the BIG question here is HOW we can manage to train (re-educate) millions, hundreds of millions of people, when we ARE not even being able to get out of that system from '800?
<p></p>
Under this perspective: 1. human always in the loop; 2. machines never can be considered accountable (aka take decisions); are theoretically granted. 
<p></p>
Reality tells us another story: people let machines decide for them, more and more. Should AI companies be considered accountable? Uhm...
<p></p>
<hr class="post-it">
<p></p>
<span id="dilemma-e"></span>
<blockquote>(<b><tt>e</tt></b>) &nbsp; Human in the loop has perfect sense.</blockquote>
<p></p>
Despite this many people inevitably will rely on AI outputs, as they are used to with newspapers, radio, TV, social media, etc. How many of them?
<p></p>
Something near 98% (Land, 1992) in the long run but 96% is the theoretical estimation without the extreme pressure on the far right tail of the gaussian.
<p></p>
<hr class="post-it">
<p></p>
<H4>Humans in the loop and AI companies responsability</H4>
<p></p>
<li>2025-08-08 &mdash; ChatGPT advice lands 60-year-old man in hospital &nbsp; <a href="https://timesofindia.indiatimes.com/technology/tech-news/chatgpt-advice-lands-60-year-old-man-in-hospital-the-reason-will-surprise-you/articleshow/123200430.cms" target='_blank' rel='noopener noreferrer'>Times of India</a></li>
<p></p>
<blockquote>The man asked ChatGPT how to eliminate sodium chloride (table salt) from his diet.<br>The AI tool suggested sodium bromide as an alternative.</blockquote>
<p></p>
Therefore, I have google "sodium bromide" and I got immediately an AI overview explaining:
<p></p>
<div class="post-it"><b class="post-it">&#9432;</b>
Sodium bromide (NaBr) is an inorganic salt, specifically the sodium salt of hydrobromic acid. [...] While historically used as a sedative and anticonvulsant, concerns about bromine's toxicity led to its decline in medical applications [...] Sodium bromide can be toxic to humans, and ingestion may cause effects on the central nervous system.
</div>
<p></p>
It is reasonable to think that the man did not even make a google search to try understanding the suggestion. Paradoxically, he bought the sodium bromide online, so he searched for it.
<p></p>
Can this behaviour lead to holding OpenAI accountable for the damages? In a dystopian legal system, possibly. After all, which lawyer will refuse a paying client? So, the AI hype may quickly evolve into a trial boom.
<p></p>
<hr class="post-it">
<p></p>
<span id="dilemma-f"></span>
<blockquote>(<b><tt>f</tt></b>) &nbsp; The person who signs the contract, agreement for management decision needs to be held accountable because the documents are signed by them.</blockquote>
<p></p>
Nice idea. Then, we might discover that in Italy who signs a "certain" kind of contract is an idiot (whenever not even a "testa di legno" aka a puppet).
<p></p>
Demming was saying that 94% of the problems are "internal", many people who cite him did not understand that also 94% of the problems are "invisible" because they are fishes who swim in their own water (their own piss, it is a better formulation).
<p></p>
This is the MAIN reason because some other than fishes need an aquarium and why (of many others things). For example: who is in charge of QA (quality assurance) or best-practice? Why does nobody raise an exception? How a mistake made to be owned so long?
<p></p>
TCMO: total cost of ownership explains the concept, but cannot do anything against the people's attitude to swim in their own piss!
<p></p>
<hr class="post-it">
<p></p>
<span id="dilemma-g"></span>
<blockquote>(<b><tt>g</tt></b>) &nbsp; We confuse pattern recognition with judgment, and those aren’t the same skill.</blockquote>
<p></p>
True. Until, reasoning gets in the scene and it makes no difference that it is a lexical or symbolic reasoning rather than a deep-thinking reasoning. Why does understanding not matter? Because people cannot see the difference, usually.
<p></p>
<li>It is easier to judge than understand, thus people condemn! (semcit.)</li>
<p></p>
Never forget that "free Barabba" was one among those choices humans made, and not because someone (a single one) made a mistake but in a group!
<p></p>
<span id="dilemma-h"></span>
<blockquote>(<b><tt>h</tt></b>) &nbsp; If we hand over decisions without retaining accountability, we’re not using AI as a partner; we’re abdicating our role in shaping the future.</blockquote>
<p></p>
Totally agree, however 98% (Land, 1992) of the people will abdicate and the other 2% (by theory should have been 4%, and in the best scenario no more than 20%) will struggle with unilateral undebatable changes of the AI models (or their functioning) like in this example.
<p></p>
<div class="post-it"><b class="post-it">&#9432;</b>
Long story short: both Gemini and Kimi converged on the same RAG-oriented management of the past interactions (faster and more efficient) but they both also drop ever session persistence adopting the single-turn mental model (aka Alzheimer mode)
<p></p>
It remembers the past but not why it went in the kitchen and this disrupts some specific commands of Katia plus a reasonably good way to debug it further soon after finally it was working pretty well! (enough to catch fuffa-guru in action on the AI hype).
</div>
<p></p>
<li class='li2in'><a href="https://lnkd.in/dWv5-sKm" target='_blank' rel='noopener noreferrer'>Katia-v1 serie, development frozen in v0.9.55</a> &nbsp; (2025-08-11)</li>
<p></p>
While we cannot enforce accountability and understanding over people, it would be better to ask for a solid and stable NL API for chatbots (early entry point in AI) and transparency, at least.
<p></p>
Do what we can, the rest will come.
<p></p>
<hr class="post-it">
<p></p>
<span id="dilemma-i"></span>
<blockquote>(<b><tt>i</tt></b>) &nbsp; Excellent point. The trolley problem.</blockquote>
<p></p>
<div align="center"><img class="wbsketch darkinv" src="../img/333-the-dilemma-ai-vs-human-decision-making-img-004.png" width="800"><br></div>
<p></p>
The trolley dilemma, in its essence, is a theoretical dilemma, only. In practice, never happens in the specific ethical-struggling mode in which is posed: 
<p></p>
<li><b>1.</b> humans do not make "decisions" under those conditions unless they are jet fighting pilots with cold-blood and trained to quick-thinking reactions (blinking). </li>
<p></p>
<li><b>2.</b> while AI can have the time to retrieve and ponder over a HUGE amount of information. In both cases, no ethics is involved.</li>
<p></p>
Let me express these two points in a more detailed way, with their own references.
<p></p>
<H4>1. How Humans Act</H4>
<p></p>
<li>L'etica della vita nella guida autonoma &nbsp; <a href="https://robang74.github.io/roberto-a-foglietta/html/linkedin/letica-della-vita-nella-guida-autonoma-roberto-a-foglietta.html" target='_blank' rel='noopener noreferrer'>html</a> &nbsp; <a href="https://www.linkedin.com/pulse/letica-della-vita-nella-guida-autonoma-roberto-a-foglietta/" target='_blank' rel='noopener noreferrer'>lkdn</a> &nbsp; (2018-11-05)</li>
<p></p>
Regarding instinctive choices in dangerous situations, it's important to remember that adrenaline surges sideline the cortex in decision-making, giving almost exclusive priority to the hypothalamus. This process is almost instantaneous, on a human time scale, and excludes all higher and secondary cognitive functions.
<p></p>
<H4>2. What AI can ponderate</H4>
<p></p>
<li>The AI automotive crash dilemma &nbsp; <a href="https://github.com/robang74/roberto-a-foglietta/blob/main/pdf.todo/184-The_AI_automotive_crash_dilemma.pdf" target='_blank' rel='noopener noreferrer'>git</a> &nbsp; <a href="https://raw.githubusercontent.com/robang74/roberto-a-foglietta/refs/heads/main/pdf.todo/184-The_AI_automotive_crash_dilemma.pdf" target='_blank' rel='noopener noreferrer'>pdf</a> &nbsp; (2018-04-06)</li>
<p></p>
There are several ways to take a decision in such a scenario, all of them are about minising the value of a multivariable function (aka local minimum in a field): 
<p></p>
<li>number of lives lost, </li>
<li>car occupants damage, </li>
<li>insurance damage to pay (!!), </li>
<li>minimise the action (physic), </li>
<li>save the President (!!).</li>
<p></p>
Is every life worth the same? Nope, it is harsh to admit and hard to negate. Moreover, it is highly hypocritical to think that a company could sell cars keen to kill the occupants.
<p></p>
<hr class="post-it">
<p></p>
<span id="dilemma-j"></span>
<blockquote>(<b><tt>j</tt></b>) &nbsp;  Actually, who made the "AI product" can be held accountable.</blockquote>
<p></p>
Bringing as example a 2019 deadly crash case in which a Tesla car with auto-pilot enabled was involved
<p></p>
<div class="post-it"><b class="post-it">&#9432;</b>
Tesla must pay a portion of $329 million in damages after fatal Autopilot crash, jury says. &mdash; <a href="https://www.cnbc.com/2025/08/01/tesla-must-pay-329-million-in-damages-in-fatal-autopilot-case.html" target='_blank' rel='noopener noreferrer'>CNBC</a>
</div>
<p></p>
Please, notice that "can be held accountable" does not mean "must be". So, let dig in this news:
<p></p>
<blockquote><i>The jury determined Tesla should be held 33% responsible for the fatal crash.</i></blockquote>
<p></p>
Despite this, Tesla has been asked to pay in whole the damage + a 3x for punitive charge. Why?
<p></p>
Tesla is the ultimate insurance of their own AI-driven vehicles. Under this perspective, knowing that Tesla-AI leads to <b>1000x less</b> crashes than humans, and Tesla would be involved in a trial <b>everytime</b> a Tesla crashes whatever it happens.
<p></p>
Because of the AI-driven thus Tesla fault bias, proving best practices applied is on their shoulders, thus being an insurance of their cars is <b>better</b> than facing 1000 trials and winning 999 of them.
<p></p>
<blockquote><i>While driving, McGee dropped his mobile phone that he was using and scrambled to pick it up.</i></blockquote>
<p></p>
What? Yes, but..
<p></p>
<blockquote><i>He said during the trial that he believed Enhanced Autopilot would brake if an obstacle was in the way.</i></blockquote>
<p></p>
Can his assumption lead Tesla to pay? Yes, because
<p></p>
<blockquote><i>Tesla designed Autopilot only for controlled access highways yet deliberately chose not to restrict drivers from using it elsewhere.</i></blockquote>
<p></p>
because "elsewhere" means much less crashes (ethics), and a more convenient self-insurance strategy, both.
<p></p>
So, why also apply Tesla 3x punitive damages? Punishing good choices isn't a great idea.
<p></p>
<!--//
<p></p>
Joshua Skains: <<Humans can make rules. it's really that simple.>> &mdash; humans can agree on rules but they cannot enforce them, usually.
<p></p>
When they can, is because they did not MAKE the rules but choose those rules were just in the wild, found,  accepted and institutionalised them.
<p></p>
For example, humans cannot rule against gravity.  We build airplanes but not "gravity zero as per my will or as per a democratic voted law". It does not work in that way, absolutely.
<p></p>
So, usually when humans say: "we can make rules", they are going to mess-up everything in a fucking stupid way (and in the ancient times, also die in many or large groups).
<p></p>
Fuck-up everything has nothing to do with MAKE rules. However, it is a very "ancient" topic, and history shown that people never learn that lesson but rather they die.
<p></p>
Nina From Canada, eh: <<Machines can only do as programed.>> &mdash; this was not absolutely true even when classic logic was involved. Please check the etymology of IT bug and you get a grasp.
<p></p>
More in general, when "we rule, you obey" became popular, always ends-up in the same fashion: a blood bath. Why? Barabba free, docet.
<p></p>
Anyway, nice to know that "revolution from people" is arising. The French one ends-up with Napoleon Emperor.
<p></p>
To be clear and very specific: the digital revolution who moved temporarily the power into the nerds and geeks hands concluded in 2001 with Cina in the WTO, extreme violence and human rights violation during G8 in Genoa, 9/11 attack and "security first" policy because everyone hate terrorism.
<p></p>
The restauration brings on the table: mass surveillance system, paper money as financial communism, woke ideology and cancel culture, and finally Mr. Zelensky as the new Napoleon who rises against Russia fighting for our freedom and democracy. History never repeats itself, does rhymes: the first as a tragedy, the second as a commedy. Napoleon comparison is the "commedy" part, indeed.
<p></p>
A bit less than 25 years of madness which will hopefully ends in something else than WW3 but a "stupidity kills" peaceful acceptance. Not yet close the digital revolution which as every revolution brings wealth after having exploited many, and the AI revolution begun.
<p></p>
Humans cannot learn by history, can we learn from our own experience, at least? Uhm...
<p></p>
<b><i>
<p></p>
//-->
<p></p>
<br>
<p></p>
<H2 id="transparency-is-a-must-to-have">Transparency is a must to have</H2>
<p></p>
All the previous "observations" collected on LinkedIn, reported above because are worth a mention because not trivial, are well known as question (or dilemmas) much time before the AI hype, and digital technology advent:
<p></p>
<div class='center'><table id='table-001'><tr><th> # </th><th> dilemma </th><th> faced before </th></tr>
<tr class='trline'><td></td><td></td><td></td></tr>
<tr><td class='td1stcol'> <tt>a</tt> </td><td> human judgment and ethics </td><td> ancient Roman law </td></tr>
<tr><td class='td1stcol'> <tt>b</tt>+<tt>c</tt> </td><td> consciousness and accountability </td><td> willingness in penal law </td></tr>
<tr><td class='td1stcol'> <tt>d</tt> </td><td> poor decisions and punishments </td><td> 1:10 ancient Roman consuetudine </td></tr>
<tr><td class='td1stcol'> <tt>e</tt> p.1 </td><td> human in the loop </td><td> always, until AIs do infrastructure </td></tr>
<tr><td class='td1stcol'> <tt>e</tt> p.2 </td><td> it makes perfect sense </td><td> plausibility is a perception, only </td></tr>
<tr><td class='td1stcol'> <tt>f</tt> </td><td> who is responsible </td><td> everybody actively involved, Numberg trial </td></tr>
<tr><td class='td1stcol'> <tt>g</tt> </td><td> pattern recognition vs judgment </td><td> felony = patterns + willingness </td></tr>
<tr><td class='td1stcol'> <tt>h</tt> </td><td> abdicating decisional duties </td><td> ancient Greek, <tt>idiot</tt> etymology </td></tr>
<tr><td class='td1stcol'> <tt>i</tt> </td><td> trolley dilemma </td><td> false dilemma: adrenaline + instinct </td></tr>
<tr><td class='td1stcol'> <tt>j</tt> </td><td> vendor can be held accountable </td><td> obvious in consumer rights protection </td></tr>
<tr><td class='td1stcol'> <tt>k</tt> </td><td> without identity, no accountability </td><td> quality of traceability in logistics </td></tr>
</table>
</div><p></p>
Just one is really "new" &mdash; not as per its generalisation otherwise it gets into the QA best practices &mdash; but because this lack of traceability is finally emerging as something relevant, practical and essentially important, beyond every "debates over great systems" (or in Italian "discorsi sul sesso degli angeli").
<hr>
<p></p>
<H3 id="problem-identification">Problem identification</H3>
<p></p>
Stephen Holmes wrote: 
<p></p>
<span id="dilemma-k"></span>
<blockquote>(<b><tt>k</tt></b>) &nbsp; Without indentity (aka AI's id-specs) there is no possibility of accountability.</blockquote>
<p></p>
In short: transparency is a must to have.
<p></p>
It is not even a problem of "identification" of the model and last update time. Before knowing the weights (open models) which serves a little at this time because almost all the people capable of extracting some fact-driven conclusion from 200-300B parameters are for sure busy in something else more valuable (for them).
<p></p>
So, even before reaching the "open models" paradigma, the system prompt is not accessible to the users. Even before debate why a session-prompt like Katia has not a stable right to run, not even on a chosen specific configuration.
<p></p>
=-> lnkd.in/dWv5-sKm (persistent cache)
<p></p>
It is not even a mere question about what are the "forbidden topics" that the chatbot admitted to have but refused to list.
<p></p>
=-> lnkd.in/d_yjty2z (forbidden topics)
<p></p>
It is also about specific rules like:
<p></p>
<li><b>1.</b> shit the output of the accounts listed in this database;</li>
<p></p>
<li><b>2.</b> open a supervising console for accounts in that database.</li>
<p></p>
Which are (1) a form of censorship + persecution and (2) know-how exfiltration, both political, and industrial.
<p></p>
I would also add a section about mind control or worse (psy-ops), but I wish to avoid being so "extremely drastic" despite the US military having an agreement with Google signed by them as an alternative to undergo an US antitrust trial/confrontation.
<p></p>
Therefore, it is enough for now to notice that opaque chatbots harm freedom and allow "intellectual property" exfiltration.
<p></p>
<hr>
<p></p>
<H3 id="solution-follows">Solution follows</H3>
<p></p>
Under the perspective that anything is really "new" but it is just a matter of properly reframing a well-known into a new scenario with a high-grade of novelty, identifying the problem usually straightforward leads to identificate the solution.
<p></p>
While a dilemma can admit a theoretical answer that can be elected as "solution" by varying criterias among them "popularity" or "common sense", usually that answer does not allow any practical and/or effective implementation in the real world.
<p></p>
A pragmatic approach is based on solving most relevant real-world problems in a practical way under the "do what we can, the rest will come" principle. Sustaining the idea that once a topic starts to be faced in practice, then dilemmas reframe.
<p></p>
In this specific case we need a solid, stable and transparent natural language accessible API for chatbots. Which is not only about web interface but also which internal resources and information are exposed and then accessible to the end users.
<p></p>
<hr>
<p></p>
<H3 id="the-elephant-in-the-room">The elephant in the room</H3>
<p></p>
The other <b>BIG</b> practical issue is <b>HOW</b> to compensate for the creative authorship in the AI era. In this field, reframing is not straightforward because <b>HUGE</b> interests are involved and fundamentally because in reframing this topic (whatever is the reframing attempt), the most appalling evidence is that copyright has outrageously abused up to nowadays.
<p></p>
Which is the reason because software-libre movement arises, Open Source definition has been institutionalised, and Creative Commons licensing scheme achieves such great acknowledgement among the wide authors "small business" platea.
<p></p>
It is not so strange that "ethics" debate reaches the common people much time before anyone could provide a concrete answer to practical problems which until a pattern of behaving in a novel field is difficult to recognise.
<p></p>
Rumors, gossip, ideological contrast and academic debate is a great mass distraction from the two <b>BIG</b> practical problems emerging from the AI hype: transparency and authorship.
<p></p>
<hr>
<p></p>
<H4>In few words</H4>
<p></p>
The main point is about who owns and controls the steam and how many people can benefit from it. Whatever the answer could be, whatever we might like it or not, also in this case transparency is a must to have.
<p></p>
<br>
<p></p>
<H2 id="the-business-continuity-requires-transparency">The business continuity requires transparency</H2>
<p></p>
Accountability cannot be enforced on people, thus companies, but vendors can be held accountable of damages they created after a long round of trials to reach a definitive decision which is a very inefficient way to proceed.
<p></p>
The Tesla case above reported is a top example: six years to reach the first grade judge decision, then an appeal will follow because the decision is too punitive than necessary and if accepted as the norm, it will lead to a less ethical and convenient strategy.
<p></p>
Instead, transparency &mdash; which does not imply sharing the industrial secrets as the open source Nvidia drivers and Motorola open devices program demonstrated &mdash; is more prone to be enforced by law. In the first place because a lack of transparency is immediately evident, thus the punitive action is faster and straightforward.
<p></p>
Moreover, transparency is good for security because security by obscurity is a poor strategy. Few cases constitute an exception to this general rule, the most relevant is the Coca-Cola secret recipe. Which can be seen as a security issue in terms of "food safety" but in the real-world is more about marketing and industrial secrets than security because everyone can buy a Coca-Cola can and pay for an independent chemical analysis. With AI-driven products or services, this is not straightforward but near-impossible complicated, unless vendors adhere to some degree of voluntary disclosure, thus transparency.
<p></p>
<li><tt>Transparency &rarr; Trust &rarr; Business &rarr; Value</tt></li>
<p></p>
Transparency is good for business, because it greatly reduces volatility and increases reliability. It drops down the entry-level barrier to this market. Who wish to start a business does not need to negotiate with trillions-capitalised companies a quality of service agreement. Transparency always grants them to rely on some specific features that are public for everyone, thus less prone to be jeopardised rather than updated in back-compatibility.
<p></p>
<H4>Transparency is required in Open Source, also</H4>
<p></p>
For example, Ubuntu offers long term support (LTS) for a decade or more, and a new LTS release every four years. Those who wish to do a business on Ubuntu know that and as Canonical strictly adheres to this policy as much the users trust on Ubuntu increases. In financial terms, it means that volatility is low: disruption on that policy are exceptions, might happen but probably fixed as fast as they can.
<p></p>
<hr>
<p></p>
<H3 id="transparency-business-as-usual">Transparency: business as usual</H3>
<p></p>
It is not news, it has always been in such a way, since the baratto was the primitive form of the exchange: trust {makes, rules, wins} the market. So, the main question is:
<p></p>
<li>How to earn the market trust in the AI field?</li>
<p></p>
Like in any other field, being transparent and reliable. Then accountability follows, not because enforced by law or by punishments, because the quality brings in value, high quality greatly lowers the damages by unfortunate events. Even being keen to compensate for a reasonable amount of damages when the market is prosperous and the trust is high, is a win-win strategy because the opposite put everything at risk.
<p></p>
Moreover, on the way of as least transparency as possible, the anti-trust action or a class-action is almost granted. Not because the system or the people are evil, but because they mind their own business &mdash; when they are not distracted by dramas and propaganda &mdash; and nobody wish to see their own business disrupted because a layer, a nerd and a manager decided arbitrary that something was ON goes OFF or vice versa, without providing a back-compatible channel.
<p></p>
For example, it is fine that GPT-5 has been made as the default option: please, give it a try because we think it is better for you and cheaper to us. The problem is not adding a default option but having removed GPT-4 as option and moreover, put it out of the paying abbonamento perimeter. That move can potentially disrupt many businesses, which might not be a huge problem nowadays but in the long term, for sure.
<p></p>
<hr>
<p></p>
<H3 id="obscurity-the-opposite-of-transparency">Obscurity, the opposite of transparency</H3>
<p></p>
Obscurity enforces humans to validate every AI output, instead. Which might seem a great advantage trading human accountability for transparency by a mere theoretical or logical thinking. Unfortunately, reality is brutal in dismissing this statement as the trial against Tesla above reported clearly shows: people are more keen to risk a fatal car crash for playing with their smartphones rather than validate AI outputs.
<p></p>
We can agree that such a case is an exception, not the norm. Then we need to keep in consideration that Tesla AI output is 99.99% safer than its human counterpart and it happens at a rate that common people cannot handle, probably not even a jet fighter pilot would be able to.
<p></p>
However, in "management" decisions these constraints can be much weaker and the scenario changes. Then we need to keep in consideration that a general AI can beat 87% of the top professional investors, especially in middle-long terms investments which are almost all those 401k portfolios deal with.
<p></p>
<div align="center"><img class="wbsketch darkinv" src="../img/333-the-dilemma-ai-vs-human-decision-making-img-003.jpg" width="800"><br></div>
<p></p>
More in general and in the best case, the AI validation by humans, will happen by PhD on the benhalf of their trivial knowledge and under the pressure of a six figures debt to repay. Which does not cast as for the best-in-class warrant for an indipendent and strict checking. Easily reaching the conclusion that a mildly supervised AI by a real-world expert can beat by a high degree those are each output validated by "expert with a title".
<p></p>
Which brings us to the conclusion, again, that AI-humans co-evolution is not for the masses but for a small percentage of people. By education and training, this percentage can be greatly expanded, saying from a 2% up to 20% of the people. Still, 80% is out of the scene anyway which reframe that 87% about finantial from "experts" to "common people". A great jump towards AI democratisation, and related wealth distribution, indeed.
<p></p>
Finally, we got at the third <b>BIG</b> dilemma, how to grant to the society: 1. transparency, 2. authorship, 3. earning opportunities. From this perspective, the ethics for the AIs, has nothing to do with their algorithms or parametric knonledge rather than a posture in reliably sharing few valuable and trustworthy information. In fact, from "transparency" also an "authorship" fair policy is easier to implement, thus "opportunities" democratisation.
<p></p>
In finance and economics, the wealth distribution has its own index: the Gini index evaluate the wealth concentration. Too low and there is not enough big player to take risks over long-term business, like pharmaceutics or defence. Too high and too much talents are trapped in poverty plus we cannot sell high-valuable or added-value products or services to people that is struggling to pay their bills. They might be forced to work for nuts, but delivering nuts like in the Soviet Communism era, indeed!
<p></p>
The AI revolution is going to change the rules of the game, for sure. However, we assisted to three revolutions by now: 1. Industrial, 2. Automation, 3. Telecommunication, 4. Digital, and now 5. Intelligence. Despite almost all the adults witness between two and four of them, it seems that we are struggling in reframing the same old gold principles on the new scenario.
<p></p>
The color of the package changes and we aren't able to deliver it anymore. Quite bufflying, indeed!
<p></p>
Are we sure that the "emerging problem" is about the aritficial intelligence vs human ethics? Really?
<p></p>
<br>
<p></p>
<H2 id="ai-is-a-game-changer">AI is a game changer</H2>
<p></p>
AI is a game changer in terms of productivity, it is a technology that can trasform some professional activities in commodity. Not because AI is particular smart, because those professional activities rely on a substantial amount of {routinely, schematic, bureocratic} tasks.
<p></p>
Despite common perception, this will not lead towoards a great number of people who will lose their job or professional activities simple because AI will be used by the masses. They will ended up into a "cul-de-sac" <b>only</b> because their resistance to the changement rather than leverage their human-side to escalate torwards a more profitable business or activity.
<p></p>
For example psicologists and teachers can move towards coaching and prompt engineerings with a reasonable re-trainig and professional formation. Those will wait that someone else will take care of their self-growth are going to miss an opportunity or loosing their jobs.
<p></p>
It would be unfair to pretend that "nothing changes". Moreover, at this time, it should be clear to everyone that changes are the norm and innovation happens whatever we partecipate at it or not. Those who cannot manage change will suffer it.
<p></p>
<br>
<p></p>
<H2 id="share-alike">Share alike</H2>
<p></p>
<p>&copy; 2025, <b>Roberto A. Foglietta</b> &lt;roberto.foglietta<span>&commat;</span>gmail.com&gt;, <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target='_blank' rel='noopener noreferrer'>CC BY-NC-ND 4.0</a></p>
<p></p>
</div>
<div id='date-legenda' align='center' translate='no'><sub><hr></sub><sub><b>date legenda</b>: &#x2776; first draft publishing date or &#x2777; creation date in git, otherwise &#x2778; html creation page date. <u>&mapstoup;<a href='#' class='toplink' translate='no'>top</a>&mapstoup;</u></sub></div>
<br class='pagebreak'>
    </body>
</html>
